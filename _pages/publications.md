---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---


Publications and Preprints 
-----


## Language Models

- <font size="4"> Embodied LLM Agents Learn to Cooperate in Organized Teams </font>  <i> Xudong Guo, <b>Kaixuan Huang</b>, Jiale Liu, Wenhui Fan, Natalia VÃ©lez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, Mengdi Wang </i>
<br/> arXiv preprint <a href="https://arxiv.org/abs/2403.12482">[link]</a>

- <font size="4"> Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications </font>  <i> Boyi Wei\*, <b>Kaixuan Huang\*</b>, Yangsibo Huang\*, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson </i>
<br/> ICML 2024 <a href="https://arxiv.org/abs/2402.05162">[link]</a> <a href="https://github.com/boyiwei/alignment-attribution-code">[Code]</a> 

- <font size="4"> Visual Adversarial Examples Jailbreak Large Language Models </font>  <i> Xiangyu Qi\*, <b>Kaixuan Huang\*</b>, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal </i>
<br/> AAAI 2024 (<b> Oral </b>) ICML2023 Adv ML workshop. (<b>Oral</b>) <a href="https://arxiv.org/abs/2306.13213">[link]</a> <a href="https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models">[Code]</a> 

- <font size="4"> Scaling In-Context Demonstrations with Structured Attention </font>  <i> Tianle Cai\*, <b>Kaixuan Huang\*</b>, Jason D. Lee, Mengdi Wang </i>
<br/> ICML 2023 Workshop on Efficient Systems for Foundation Models.  <a href="https://arxiv.org/abs/2307.02690">[link]</a>

## Diffusion Models

- <font size="4"> Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement </font>  <i> Hui Yuan, <b>Kaixuan Huang</b>, Chengzhuo Ni, Minshuo Chen, Mengdi Wang</i>
<br/> In <i>Advances in Neural Information Processing Systems</i> (NeurIPS), 2023. <a href="https://arxiv.org/abs/2307.07055">[link]</a> <a href="https://github.com/Kaffaljidhmah2/RCGDM">[Code]</a> 

- <font size="4"> Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data </font>  <i> Minshuo Chen\*, <b>Kaixuan Huang\*</b>, Tuo Zhao, Mengdi Wang </i>
<br/> In <i>International Conference on Machine Learning</i> (ICML), 2023. <a href="https://arxiv.org/abs/2302.07194">[link]</a> 

## AI for Sciences

- <font size="4"> A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions </font>  <i> Yanyi Chu\*, Dan Yu\*, Yupeng Li, <b>Kaixuan Huang</b>, Yue Shen, Le Cong, Jason Zhang, Mengdi Wang</i>
<br/> Nature Machine Intelligence (2024) <a href="https://www.nature.com/articles/s42256-024-00823-9">[link]</a> 

- <font size="4"> Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources </font>  <i> Yikuan Li\*, Chengsheng Mao\*, <b>Kaixuan Huang\*</b>, Hanyin Wang\*, Zheng Yu\*, Mengdi Wang, Yuan Luo </i>
<br/> arxiv preprint <a href="https://arxiv.org/abs/2309.08560">[link]</a>  

- <font size="4"> Deep Reinforcement Learning for Cost-Effective Medical Diagnosis </font>  <i> Zheng Yu\*, Yikuan Li\*, Joseph Kim\*, <b>Kaixuan Huang\*</b>, Yuan Luo, Mengdi Wang </i>
<br/> In <i>International Conference on Learning Representations</i> (ICLR), 2023. <a href="https://openreview.net/forum?id=0WVNuEnqVu">[link]</a> 


## Theories on Optimization, Deep Learning, and Reinforcement Learning

- <font size="4"> Going Beyond Linear RL: Sample Efficient Neural Function Approximation </font>  <i> Baihe Huang, <b>Kaixuan Huang</b>, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, Jiaqi Yang </i> (alphabetical)
<br/> In <i>Advances in Neural Information Processing Systems</i> (NeurIPS), 2021. <a href="https://arxiv.org/abs/2107.06466">[link]</a> 


- <font size="4"> Optimal Gradient-based Algorithms for Non-concave Bandit Optimization </font>  <i> Baihe Huang, <b>Kaixuan Huang</b>, Sham M. Kakade, Jason D. Lee, Qi Lei, Runzhe Wang, Jiaqi Yang </i> (alphabetical)
<br/> In <i>Advances in Neural Information Processing Systems</i> (NeurIPS), 2021. <a href="https://arxiv.org/abs/2107.04518">[link]</a> 

- <font size="4"> A Short Note on the Relationship of Information Gain and Eluder Dimension </font>  <i> <b>Kaixuan Huang</b>, Sham M. Kakade, Jason D. Lee, Qi Lei </i> (alphabetical)
<br/> ICML2021 Workshop on Reinforcement Learning Theory. <a href="https://arxiv.org/abs/2107.02377">[link]</a> 

- <font size="4"> Fast Federated Learning in the Presence of Arbitrary Device Unavailability </font>  <i> Xinran Gu\*, <b>Kaixuan Huang\*</b>, Jingzhao Zhang, Longbo Huang </i>
<br/> In <i>Advances in Neural Information Processing Systems</i> (NeurIPS), 2021. <a href="https://arxiv.org/abs/2106.04159">[link]</a> 

- <font size="4"> Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? --- A Neural Tangent Kernel Perspective </font>
<i><b>Kaixuan Huang\*</b>, Yuqing Wang\*, Molei Tao, Tuo Zhao </i>
<br/> In <i>Advances in Neural Information Processing Systems</i> (NeurIPS), 2020. <a href="https://proceedings.neurips.cc//paper/2020/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html">[link]</a>

- <font size="4"> On the Convergence of FedAvg on Non-IID Data </font>
<i>Xiang Li\*, <b>Kaixuan Huang\*</b>, Wenhao Yang\*, Shusen Wang, Zhihua Zhang </i>
<br/> In <i>International Conference on Learning Representations</i> (ICLR), 2020. (<b>Oral Presentation</b>) <a href="https://openreview.net/forum?id=HJxNAnVtDS">[link]</a>
